{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-Dimensional Estimator For Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents an extension for computing the estimator of mutual information between 2 random variables to m random variables. Sklearn has a function for such a 2-dimensional problem called `mutual_info_regression`. The source code is modified such that we can take a $p$ dimensional design matrix and estimate the mutual information between $m$ predictors and the target. The estimator can be found in the following paper: https://journals.aps.org/pre/pdf/10.1103/PhysRevE.69.066138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information of Two Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(X;Y) = \\int \\int p_{X, Y}(x, y) \\log \\frac{p_{X, Y}(x, y)}{p_X(x)p_Y(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this takes the shape of the [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). Also recall that if $X$ and $Y$ are completely independent, then the joint probability distribution is the product of the marginals. In this case, indepedent continuous random variables yield a mutual information of 0, which can be observed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 2-Dimensional Estimator of Mutual Information\n",
    "\n",
    "$$I(X, Y) = \\psi(k) + \\psi(N) - \\langle \\psi(n_{x_1 + 1}) + \\psi(n_{y + 1})\\rangle$$\n",
    "\n",
    "Suppose $Z = (X, Y)$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\psi(x)$ is the digamma function\n",
    "\n",
    "- $k$ is the number of nearest neighbors\n",
    "\n",
    "- $m$ is the number if dimensions\n",
    "\n",
    "- $N$ is the number of samples\n",
    "\n",
    "- $n_{x_i}$ is the number of points $x_j$ strictly less than the radius, where the radius is the distance from $z_i$ to its $k^{th}$ neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the paper, the authors propose an estimator from a nearest-neighbor approach which diverges from the traditional binning approach. They then briefly mention an extension for an M-dimensional estiamtor, which is given below.\n",
    "\n",
    "### An M-Dimensional Estimator of Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(X_1, X_2,...,X_m) = \\psi(k) + (m-1)\\psi(N) - \\langle \\psi(n_{x_1}) + \\psi(n_{x_2}) + ... + \\psi(n_{x_m}) \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $Z = (X_1, X_2,...,X_m)$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\psi(x)$ is the digamma function\n",
    "\n",
    "- $k$ is the number of nearest neighbors\n",
    "\n",
    "- $m$ is the number if dimensions\n",
    "\n",
    "- $N$ is the number of samples\n",
    "\n",
    "- $n_{x_i}$ is the number of points $x_j$ strictly less than the radius, where the radius is the distance from $z_i$ to its $k^{th}$ neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The code below modifies the source code from sklearn's `mutual_info_regression` for a 2-dimensional estimator to correspond to the M-dimensional estimator given in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from scipy.special import digamma\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi_cc(X, y, N, n_neighbors):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Xy = np.hstack((X, y))\n",
    "\n",
    "    nn = NearestNeighbors(metric='chebyshev', n_neighbors=n_neighbors)\n",
    "\n",
    "    nn.fit(Xy)\n",
    "    radius = nn.kneighbors()[0]\n",
    "    radius = np.nextafter(radius[:, -1], 0)\n",
    "\n",
    "    n_i = []\n",
    "    for i in range(Xy.shape[1]):\n",
    "        \n",
    "        Xy_i = Xy[ : , i].reshape((-1, 1))\n",
    "        kd = KDTree(Xy_i, metric='chebyshev')\n",
    "        n_i.append(kd.query_radius(Xy_i, radius, count_only=True, return_distance=False) - 1)\n",
    "\n",
    "    digamma_mean = lambda x : np.mean(digamma(x + 1))\n",
    "    mi = digamma(n_neighbors) + m * digamma(N) - sum(list(map(digamma_mean, n_i)))\n",
    "\n",
    "    return max(0, mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_dim_mi(X, y, m = 2, n_neighbors = 3):\n",
    "    \n",
    "    p = X.shape[1]\n",
    "    all_pairs = list(combinations(range(p), m))\n",
    "    \n",
    "    y = y.reshape((-1, 1))\n",
    "    N = y.size\n",
    "    \n",
    "    mis = [compute_mi_cc(X[ : , pair], y, N, n_neighbors) for pair in all_pairs]\n",
    "    \n",
    "    return mis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-way MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0 and x1    2.075973\n",
       "x0 and x2    0.316282\n",
       "x0 and x3    0.318014\n",
       "x0 and x4    0.326788\n",
       "x1 and x2    0.318247\n",
       "x1 and x3    0.318675\n",
       "x1 and x4    0.326551\n",
       "x2 and x3    0.000718\n",
       "x2 and x4    0.000000\n",
       "x3 and x4    0.005980\n",
       "Name: MI Scores, dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 5\n",
    "n = 10000\n",
    "X = np.random.normal(size = (n, p))\n",
    "y = X[ : , 0] * X[ : , 1] + np.random.normal(scale = 0.1, size = n)\n",
    "\n",
    "m = 2\n",
    "mi_scores = m_dim_mi(X, y, m = m)\n",
    "all_pairs = list(combinations(range(p), m)) \n",
    "\n",
    "indices = ['x' + str(pair[0]) + ' and x' + str(pair[1]) for pair in all_pairs]\n",
    "pd.Series(mi_scores, name=\"MI Scores\", index=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-way MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0    0.331598\n",
       "x1    0.327218\n",
       "x2    0.000000\n",
       "x3    0.002460\n",
       "x4    0.011109\n",
       "Name: MI Scores, dtype: float64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape((-1, 1))\n",
    "Xy = np.hstack((X, y))\n",
    "df = pd.DataFrame(Xy)\n",
    "df.columns = ['x' + str(i) for i in range(p)] + ['y']\n",
    "y = df.pop('y')\n",
    "discrete_features = df.dtypes == int\n",
    "\n",
    "mi_scores = mutual_info_regression(df, y, discrete_features=discrete_features)\n",
    "pd.Series(mi_scores, name=\"MI Scores\", index=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
